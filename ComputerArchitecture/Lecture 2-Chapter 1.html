<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.12 (457936)"/><meta name="altitude" content="173.5879669189453"/><meta name="author" content="chinibottle@gmail.com"/><meta name="created" content="2018-01-25 20:03:55 +0000"/><meta name="latitude" content="32.84293658761779"/><meta name="longitude" content="-96.78271874224073"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-02-01 21:15:58 +0000"/><title>Lecture 2-Chapter 1</title></head><body><div><img src="Lecture%202-Chapter%201.html.resources/Screen%20Shot%202018-01-25%20at%2014.08.50.png" height="638" width="968"/><br/></div><div><ol start="1"><li><br/></li><ol><li>Multiple chips turn into one today.</li><li>2 cores with shared cache run fast</li><li>L1, L2 or more maybe on the same chip</li><li>Backside Bus-&gt;fast speed of CPU</li><li>Maybe a single has own cache</li><li>Frontside Bus-&gt; high bandwidth bus attached to main memory &amp; GPU through controller called North Bridge</li><li>South Bridge is used to connect to things like printers</li><li>Bridges are used to connect buses with different capacities</li></ol></ol><div><img src="Lecture%202-Chapter%201.html.resources/Screen%20Shot%202018-01-25%20at%2014.18.04.png" height="204" width="836"/><br/></div></div><div><ol start="4"><li> </li><ol><li>A shared bus can serve the Interconnection System to share the memory</li><li>But the problem is Collision. NOC(Network on chips) is the solution.</li><ol><li>This can handle many more processors than a bus. </li><li>Better reliability.</li><li>Quickpath Interaction: Very Fast; But the scale cannot be too much</li></ol></ol><li>Term:</li><ol><li>core of CPU: hardware that is capable of sequencing &amp; executing instructions in a thread</li><li>multi-threaded core: a core that can execute multiple threads “at the same time” where each thread runs in a hardware thread context in the core. Allows us to do fast context switching.</li><li>TLB miss: (virtual memory) let the next thread to work(switch)=&gt; when this happens, we can switch entirely different set of registers</li><li>chip multiprocessor(CMP): multiprocessor connected to do the same work</li><ol><li>Before, use clock speed to rate processors ($$$<span style="color: rgb(255, 38, 0);">need to know: nano-second, frequency=1/period</span>)</li><li>$$$First Question: Given frequency what is the period?</li><li>$$$Second Question: Given period what is the frequency?</li><li>What components that affect the clock frequency?</li><ol><li>Technology node:</li><ol><li>Switching speed of components of transistors</li><li>Wire delay(Mainly)</li></ol><li>The pipeline depth: 5 stages; cannot increase to too many</li><ol><li>overhead due to adding pipeline registers</li><li>often takes about 10 levels of logic to accomplish useful work</li></ol><li>Circuit designs: FinFEFs or better routing algorithms</li></ol></ol></ol></ol><div><br/></div></div><div><ol start="6"><li>Where do performance improvements come from?=&gt;Parallelism and Guessing</li><ol><li>Historically, processors have been scalar processors instead of a vector or matrix; we improved by adding a pipeline; After that, we took advantage of instruction level parallelism</li><ol><li>Issues occur due to load balancing between stages</li><li>Stalls due to data dependencies that could not be addressed with data forwarding</li></ol><li>With the old in-order pipeline, speed is mainly limited by the stages</li><ol><li>superscalar processors=&gt;issue multiple instructions at the same clock cycle=&gt;benefits a lot when combined with out of-order execution</li><li>Parallel processors</li><li>Vector machines or array processors=&gt;operate on multiple operands</li><li>But about the data sharing issue maybe happen sometimes</li></ol></ol><li>Performance Measures:</li><ol><li>Real time systems:</li><ol><li>Hard real time: I have catastrophic failure if I miss the deadline=&gt;Process control</li><li>Firm real time: I can occasionally miss a deadline but it will degrade performance or if result is too late it’s useless=&gt;Sound or movie</li><li>Soft real time: Missing deadlines degrade performance but result are useful even if I’m late=&gt; Data analysis later</li></ol><li>Another way of categorizing this is “response time” time between the start to the completion of a task=&gt;I/O, cpu, memory access….</li><ol><li>Performance = 1/execution time</li><li>Speedup of processor A over processor B is the ratio of the execution times=ExecTime B/ExecTime A</li></ol></ol></ol></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div></body></html>