<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.12 (457936)"/><meta name="altitude" content="172.7923736572266"/><meta name="author" content="chinibottle@gmail.com"/><meta name="created" content="2018-02-06 23:37:59 +0000"/><meta name="latitude" content="32.84294092624874"/><meta name="longitude" content="-96.78274637835588"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-02-09 00:15:16 +0000"/><title>Chapter 20</title></head><body><div><ol start="1"><li>Basic crawler operation</li><ol><li>Starting point</li><li>Repeat</li></ol><li>Assumption: The web is well linked</li><li>Scale: need to sub-select</li><li>Politeness: be nice to the server</li><li>Duplicates: spam or similar</li><li>Freshness</li><li>Robots.txt is the rule(recommendation) for crawlers</li><ol><li>E.g when to crawl the SMU, first thing is to get the robots.txt</li></ol><li>&lt;meta name=“ROBOTS” content=“NOINDEX, NOFOLLOW(if there is an index linked with, don’t follow)”&gt;</li><li>Sitemap(sitemap.xml): shows change frequency</li><li>Distributed operation: multi-thread work</li><li>Scalable: grow the crawl rate</li><li>Fetch pages of higher quality first: the most current pages</li><li>Fetch(if we can parse the web address to IP and cache it, then it will be more efficient)-&gt;Parse-&gt;docFingerPrint-&gt;URLFilter-&gt;eliminate duplicate URL-&gt;URL Frontier Queue-&gt;Fetch………</li><li>URL Filter:</li><ol><li>“href”</li><li>parse the relative to absolute link by the current URL</li><li>Partition hosts(put web pages on the distributing server) being crawled into nodes<span style="color: rgb(255, 38, 0);">???</span></li><ol><li>be polite, don’t be too frequent=&gt;host splitter</li></ol></ol><li>Mercator URL frontier:</li><ol><li>one host only hold by one back queue</li></ol><li>Spider trap: server that generates an infinite sequence of linked pages</li><li><br/></li></ol><div><br/></div></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div></body></html>