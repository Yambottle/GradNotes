<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><meta name="exporter-version" content="Evernote Mac 7.12 (457936)"/><meta name="altitude" content="176"/><meta name="author" content="chinibottle@gmail.com"/><meta name="created" content="2018-02-15 18:46:18 +0000"/><meta name="latitude" content="32.8605910770013"/><meta name="longitude" content="-96.76546639301974"/><meta name="source" content="desktop.mac"/><meta name="updated" content="2018-02-27 23:21:58 +0000"/><title>Chapter 6</title></head><body><div><ol start="1"><li>Problem with boolean search: result too few or too many</li><ol><li>use rank retrieval to solve</li><ol><li>relevant rank</li><li>assign a score to each query-document pair (document comparison)</li></ol><li>score: frequency</li><ol><li>jaccard(A, B) = A and B/ A or B is ok, but it doesn’t show term frequency(occur many times)</li><li>First, need to normalize </li><ol><li>e.g ‘one’ 2 times in 4 word; 4 times in 8 words</li><li>Cosine Normalization</li></ol><li>Count Matrix</li><li>Bag of words model: set with no order</li><li>Term Frequency: tf: number of times that t occurs in each doc</li><li>Log Frequency Weighting:</li><ol><li>w = 1+log10 tf (tf&gt;0)</li><li>w = 0 (otherwise)</li><li>Score: sum over all the w of terms t in both q and d (intersection)</li></ol><li>Frequency in document vs collection</li><ol><li>frequency in collection: <span style="color: rgb(255, 38, 0);">the number of terms</span> shown in all documents</li></ol><li>Rare terms are high weight (more significant); Frequent terms are less informative</li><li>Document frequency: <span style="color: rgb(255, 38, 0);">the number of all documents</span> in the collection that the term occurs in.</li><li>Inverse document frequency: idf: log10 (N/df) (N is the number of documents in the collection)</li><ol><li>idf has little effect on ranking for one-term queries</li></ol><li>TF-IDF weighting: w = tf * idf</li><ol><li>increase with number of occurrences within a document(tf)</li><li>increase with the rarity of the term in the collection(idf)</li><li>weighting notation:</li><ol><li>LTC.LTC</li><li>termf, docf, cosine</li><li>ddd.ppp</li></ol></ol><li>Progression:</li><ol><li>Binary-&gt;count(tf)-&gt;weight matrix(tf-idf), but we need to normalize</li></ol><li>Treat documents as vectors</li><ol><li>terms are axes</li><li>documents maybe not contain all the terms, so the vector is very sparse</li></ol><li>Treat queries as vectors</li><ol><li>so we can do vector comparison</li></ol><li>How to formalize vector space similarity?</li><ol><li>First cut: end point distance; but how about same angle; Euclidean distance is large for vectors of different lengths, it is bad</li><li>to compare the angle of cosine</li><li>we use L2 norm to normalize all the vectors=&gt; shorter and longer documents have the same order of magnitude</li><ol><li>e.g 115=&gt;115/sqrt(115*115+10*10+2*2)</li></ol><li>then the cos = q dot d(the length is one)</li><li>CosineScore=&gt;find the rank</li></ol><li>We can weight on queries or documents</li><ol><li>Inc.Itn</li></ol><li>we use weighting and similarity to rank the results</li><li>Zones: like title, author, date……(weight)</li><ol><li>encode: ZoneScore algorithm</li></ol></ol></ol></ol><div><br/></div></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div><div><br/></div></body></html>